import os
import logging
from flask import Flask, request, jsonify
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain import hub
from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.schema import Document
from pydantic import BaseModel, Field

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(
    filename="flask.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Flask ê¸°ë³¸ ë¡œê·¸ë„ íŒŒì¼ì— ê¸°ë¡ë˜ë„ë¡ ì„¤ì •
log = logging.getLogger("werkzeug")
log.setLevel(logging.INFO)
handler = logging.FileHandler("flask.log")
log.addHandler(handler)

logger.info("Flask RAG ì„œë²„ ì‹œì‘!")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["TAVILY_API_KEY"] = "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"

# ë°ì´í„° ë¡œë”© ë° ì¸ë±ì‹±
urls = [
    "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",
]

logger.info("ì›¹ ë¬¸ì„œ ë¡œë”© ì‹œì‘...")
docs = [WebBaseLoader(url).load() for url in urls]
docs_list = [item for sublist in docs for item in sublist]

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)
doc_splits = text_splitter.split_documents(docs_list)

vectorstore = Chroma.from_documents(
    documents=doc_splits,
    collection_name="rag-chroma",
    embedding=OllamaEmbeddings(model="llama2"),
)
retriever = vectorstore.as_retriever()
logger.info("ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ.")

# LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
llm = OllamaLLM(model="llama2", temperature=0)

class GradeDocuments(BaseModel):
    binary_score: str = Field(description="Documents are relevant to the question, 'yes' or 'no'")

grade_parser = PydanticOutputParser(pydantic_object=GradeDocuments)

prompt_rag = hub.pull("rlm/rag-prompt")
rag_chain = prompt_rag | llm | StrOutputParser()

question_rewriter = ChatPromptTemplate.from_messages([
    ("system", "You are a question re-writer that optimizes queries for web search."),
    ("human", "Here is the initial question: {question} \n Formulate an improved question."),
]) | llm | StrOutputParser()

web_search_tool = TavilySearchResults(k=3)

# ğŸ“Œ **grade_prompt ìˆ˜ì • (LLMì´ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ë„ë¡ ë³€ê²½)**
system_grade = """
You are a grader assessing relevance of a retrieved document to a user question.
Your output must be a JSON object in the following format:
{
  "binary_score": "yes" or "no"
}
If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant with "yes".
Otherwise, return "no".
Your response must be valid JSON, without extra explanation.
"""

grade_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_grade),
        ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
    ]
)

# í•¨ìˆ˜ ì •ì˜
def retrieve(question):
    logger.info(f"ì§ˆë¬¸ ìˆ˜ì‹ : {question}")
    question_encoded = question.encode("utf-16", "surrogatepass").decode("utf-16")
    documents = retriever.invoke(question_encoded)
    logger.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(documents)}")
    return documents, question

def generate(documents, question):
    logger.info("ì‘ë‹µ ìƒì„± ì‹œì‘")
    generation = rag_chain.invoke({"context": documents, "question": question})
    logger.info(f"ìƒì„±ëœ ì‘ë‹µ: {generation}")
    return generation

def grade_documents(documents, question):
    logger.info("ë¬¸ì„œ ìœ íš¨ì„± í‰ê°€ ì‹œì‘")
    filtered_docs = []
    web_search = "No"
    for d in documents:
        try:
            llm_output = llm.invoke(grade_prompt.format(question=question, document=d.page_content))
            score = grade_parser.parse(llm_output)  # JSONìœ¼ë¡œ ë³€í™˜ëœ ê²°ê³¼ ì²˜ë¦¬
            grade = score.binary_score
        except Exception as e:
            logger.error(f"LLM ì¶œë ¥ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"LLM ì¶œë ¥: {llm_output if 'llm_output' in locals() else 'N/A'}")
            grade = "no"
        if grade == "yes":
            logger.info("ë¬¸ì„œ ê´€ë ¨ì„± ìˆìŒ")
            filtered_docs.append(d)
        else:
            logger.info("ë¬¸ì„œ ê´€ë ¨ì„± ì—†ìŒ, ì›¹ ê²€ìƒ‰ ì§„í–‰")
            web_search = "Yes"
    return filtered_docs, question, web_search

def transform_query(question, documents):
    logger.info("ì§ˆë¬¸ ê°œì„  ì§„í–‰")
    better_question = question_rewriter.invoke({"question": question})
    logger.info(f"ê°œì„ ëœ ì§ˆë¬¸: {better_question}")
    return better_question, documents

# ğŸ“Œ **ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ í•´ê²° (dict â†’ str ë³€í™˜)**
def web_search(question, documents):
    logger.info("ì›¹ ê²€ìƒ‰ ì‹œì‘")
    docs = web_search_tool.invoke({"query": question})
    
    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    web_results = "\n".join([d["content"] if isinstance(d, dict) else str(d) for d in docs])
    
    web_results = Document(page_content=web_results)
    documents.append(web_results)
    logger.info("ì›¹ ê²€ìƒ‰ ì™„ë£Œ, ë¬¸ì„œ ì¶”ê°€")
    return documents, question

def decide_to_generate(web_search):
    if web_search == "Yes":
        logger.info("ë¬¸ì„œê°€ ì ì ˆí•˜ì§€ ì•Šì•„ ì§ˆë¬¸ ì¬ì‘ì„± ì§„í–‰")
        return "transform_query"
    else:
        logger.info("ë¬¸ì„œê°€ ì¶©ë¶„í•˜ë¯€ë¡œ ë°”ë¡œ ì‘ë‹µ ìƒì„±")
        return "generate"

# ì‹¤í–‰ í•¨ìˆ˜
def rag_pipeline(question):
    logger.info("RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘")
    try:
        documents, question = retrieve(question)
        filtered_docs, question, web_search_result = grade_documents(documents, question)
        decision = decide_to_generate(web_search_result)

        if decision == "transform_query":
            better_question, documents = transform_query(question, filtered_docs)
            documents, question = web_search(better_question, documents)
            generation = generate(documents, question)
        else:
            generation = generate(filtered_docs, question)
        
        logger.info("RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ")
        return generation
    except Exception as e:
        logger.error(f"RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜¤ë¥˜ ë°œìƒ"

# Flask API ì„¤ì •
app = Flask(__name__)

@app.route("/ask", methods=["POST"])
def ask_question():
    """POSTë¡œ {"question": "..."} í˜•íƒœì˜ JSONì„ ë°›ìœ¼ë©´, RAG íŒŒì´í”„ë¼ì¸ ì‘ë‹µì„ ë°˜í™˜"""
    data = request.get_json()
    user_question = data.get("question", "")
    logger.info(f"API ìš”ì²­ ìˆ˜ì‹ : {user_question}")
    answer = rag_pipeline(user_question)
    logger.info(f"API ì‘ë‹µ ë°˜í™˜: {answer}")
    return jsonify({"answer": answer})

if __name__ == "__main__":
    logger.info("Flask ì„œë²„ ì‹œì‘")
    app.run(host="0.0.0.0", port=5000, debug=True)

